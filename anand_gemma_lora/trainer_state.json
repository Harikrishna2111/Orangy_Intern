{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.390888541123283,
  "eval_steps": 500,
  "global_step": 4000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.01738828029907842,
      "grad_norm": 3.1012985706329346,
      "learning_rate": 0.00019893370421882246,
      "loss": 2.1588,
      "step": 50
    },
    {
      "epoch": 0.03477656059815684,
      "grad_norm": 1.8049442768096924,
      "learning_rate": 0.00019777468706536858,
      "loss": 1.9005,
      "step": 100
    },
    {
      "epoch": 0.05216484089723526,
      "grad_norm": 1.9462350606918335,
      "learning_rate": 0.0001966156699119147,
      "loss": 1.8229,
      "step": 150
    },
    {
      "epoch": 0.06955312119631368,
      "grad_norm": 2.104783058166504,
      "learning_rate": 0.00019545665275846083,
      "loss": 1.7828,
      "step": 200
    },
    {
      "epoch": 0.0869414014953921,
      "grad_norm": 2.3752517700195312,
      "learning_rate": 0.00019429763560500695,
      "loss": 1.7394,
      "step": 250
    },
    {
      "epoch": 0.10432968179447052,
      "grad_norm": 2.492658853530884,
      "learning_rate": 0.0001931386184515531,
      "loss": 1.6789,
      "step": 300
    },
    {
      "epoch": 0.12171796209354895,
      "grad_norm": 3.0128567218780518,
      "learning_rate": 0.00019197960129809923,
      "loss": 1.6439,
      "step": 350
    },
    {
      "epoch": 0.13910624239262737,
      "grad_norm": 2.658242702484131,
      "learning_rate": 0.00019082058414464535,
      "loss": 1.6259,
      "step": 400
    },
    {
      "epoch": 0.1564945226917058,
      "grad_norm": 3.17661452293396,
      "learning_rate": 0.00018966156699119148,
      "loss": 1.5902,
      "step": 450
    },
    {
      "epoch": 0.1738828029907842,
      "grad_norm": 2.8434364795684814,
      "learning_rate": 0.0001885025498377376,
      "loss": 1.5481,
      "step": 500
    },
    {
      "epoch": 0.19127108328986264,
      "grad_norm": 3.900380849838257,
      "learning_rate": 0.00018734353268428373,
      "loss": 1.5313,
      "step": 550
    },
    {
      "epoch": 0.20865936358894105,
      "grad_norm": 2.8485987186431885,
      "learning_rate": 0.00018618451553082988,
      "loss": 1.5112,
      "step": 600
    },
    {
      "epoch": 0.22604764388801948,
      "grad_norm": 4.185028553009033,
      "learning_rate": 0.000185025498377376,
      "loss": 1.4564,
      "step": 650
    },
    {
      "epoch": 0.2434359241870979,
      "grad_norm": 4.458552837371826,
      "learning_rate": 0.00018386648122392212,
      "loss": 1.4482,
      "step": 700
    },
    {
      "epoch": 0.2608242044861763,
      "grad_norm": 3.9057090282440186,
      "learning_rate": 0.00018270746407046825,
      "loss": 1.4133,
      "step": 750
    },
    {
      "epoch": 0.27821248478525473,
      "grad_norm": 3.5153982639312744,
      "learning_rate": 0.00018154844691701437,
      "loss": 1.4114,
      "step": 800
    },
    {
      "epoch": 0.29560076508433314,
      "grad_norm": 4.903507709503174,
      "learning_rate": 0.0001803894297635605,
      "loss": 1.3766,
      "step": 850
    },
    {
      "epoch": 0.3129890453834116,
      "grad_norm": 4.8911004066467285,
      "learning_rate": 0.00017923041261010665,
      "loss": 1.3274,
      "step": 900
    },
    {
      "epoch": 0.33037732568249,
      "grad_norm": 3.797429084777832,
      "learning_rate": 0.00017807139545665277,
      "loss": 1.3272,
      "step": 950
    },
    {
      "epoch": 0.3477656059815684,
      "grad_norm": 3.7655813694000244,
      "learning_rate": 0.0001769123783031989,
      "loss": 1.26,
      "step": 1000
    },
    {
      "epoch": 0.3651538862806468,
      "grad_norm": 3.59390926361084,
      "learning_rate": 0.00017575336114974502,
      "loss": 1.2741,
      "step": 1050
    },
    {
      "epoch": 0.3825421665797253,
      "grad_norm": 4.851850986480713,
      "learning_rate": 0.00017459434399629114,
      "loss": 1.219,
      "step": 1100
    },
    {
      "epoch": 0.3999304468788037,
      "grad_norm": 4.121377944946289,
      "learning_rate": 0.00017343532684283727,
      "loss": 1.198,
      "step": 1150
    },
    {
      "epoch": 0.4173187271778821,
      "grad_norm": 7.606426239013672,
      "learning_rate": 0.00017227630968938342,
      "loss": 1.1752,
      "step": 1200
    },
    {
      "epoch": 0.4347070074769605,
      "grad_norm": 5.139238357543945,
      "learning_rate": 0.00017111729253592954,
      "loss": 1.1454,
      "step": 1250
    },
    {
      "epoch": 0.45209528777603897,
      "grad_norm": 5.24962854385376,
      "learning_rate": 0.00016995827538247567,
      "loss": 1.1407,
      "step": 1300
    },
    {
      "epoch": 0.4694835680751174,
      "grad_norm": 4.667621612548828,
      "learning_rate": 0.00016879925822902182,
      "loss": 1.0907,
      "step": 1350
    },
    {
      "epoch": 0.4868718483741958,
      "grad_norm": 4.829826354980469,
      "learning_rate": 0.00016764024107556792,
      "loss": 1.1151,
      "step": 1400
    },
    {
      "epoch": 0.5042601286732742,
      "grad_norm": 5.6470561027526855,
      "learning_rate": 0.00016648122392211404,
      "loss": 1.0816,
      "step": 1450
    },
    {
      "epoch": 0.5216484089723527,
      "grad_norm": 5.105477809906006,
      "learning_rate": 0.0001653222067686602,
      "loss": 1.0394,
      "step": 1500
    },
    {
      "epoch": 0.539036689271431,
      "grad_norm": 4.874951362609863,
      "learning_rate": 0.00016416318961520632,
      "loss": 1.0526,
      "step": 1550
    },
    {
      "epoch": 0.5564249695705095,
      "grad_norm": 5.602859973907471,
      "learning_rate": 0.00016300417246175244,
      "loss": 1.0053,
      "step": 1600
    },
    {
      "epoch": 0.5738132498695879,
      "grad_norm": 5.464957237243652,
      "learning_rate": 0.0001618451553082986,
      "loss": 1.0027,
      "step": 1650
    },
    {
      "epoch": 0.5912015301686663,
      "grad_norm": 4.911941051483154,
      "learning_rate": 0.00016070931849791376,
      "loss": 0.9794,
      "step": 1700
    },
    {
      "epoch": 0.6085898104677447,
      "grad_norm": 6.706748008728027,
      "learning_rate": 0.00015955030134445989,
      "loss": 0.9748,
      "step": 1750
    },
    {
      "epoch": 0.6259780907668232,
      "grad_norm": 5.677081108093262,
      "learning_rate": 0.00015839128419100604,
      "loss": 0.953,
      "step": 1800
    },
    {
      "epoch": 0.6433663710659016,
      "grad_norm": 6.151584625244141,
      "learning_rate": 0.00015723226703755216,
      "loss": 0.9363,
      "step": 1850
    },
    {
      "epoch": 0.66075465136498,
      "grad_norm": 5.072002410888672,
      "learning_rate": 0.00015607324988409828,
      "loss": 0.9458,
      "step": 1900
    },
    {
      "epoch": 0.6781429316640585,
      "grad_norm": 6.177414417266846,
      "learning_rate": 0.00015491423273064444,
      "loss": 0.8892,
      "step": 1950
    },
    {
      "epoch": 0.6955312119631368,
      "grad_norm": 5.240655899047852,
      "learning_rate": 0.00015375521557719056,
      "loss": 0.9093,
      "step": 2000
    },
    {
      "epoch": 0.7129194922622153,
      "grad_norm": 6.706429958343506,
      "learning_rate": 0.00015259619842373666,
      "loss": 0.8674,
      "step": 2050
    },
    {
      "epoch": 0.7303077725612936,
      "grad_norm": 6.653132915496826,
      "learning_rate": 0.0001514371812702828,
      "loss": 0.8672,
      "step": 2100
    },
    {
      "epoch": 0.7476960528603721,
      "grad_norm": 5.365699291229248,
      "learning_rate": 0.00015027816411682893,
      "loss": 0.8514,
      "step": 2150
    },
    {
      "epoch": 0.7650843331594506,
      "grad_norm": 5.351472854614258,
      "learning_rate": 0.00014911914696337506,
      "loss": 0.8514,
      "step": 2200
    },
    {
      "epoch": 0.7824726134585289,
      "grad_norm": 6.209513187408447,
      "learning_rate": 0.0001479601298099212,
      "loss": 0.8637,
      "step": 2250
    },
    {
      "epoch": 0.7998608937576074,
      "grad_norm": 6.146225452423096,
      "learning_rate": 0.00014680111265646733,
      "loss": 0.8465,
      "step": 2300
    },
    {
      "epoch": 0.8172491740566858,
      "grad_norm": 5.576779365539551,
      "learning_rate": 0.00014564209550301346,
      "loss": 0.8135,
      "step": 2350
    },
    {
      "epoch": 0.8346374543557642,
      "grad_norm": 5.213556289672852,
      "learning_rate": 0.00014448307834955958,
      "loss": 0.8193,
      "step": 2400
    },
    {
      "epoch": 0.8520257346548427,
      "grad_norm": 5.058548927307129,
      "learning_rate": 0.0001433240611961057,
      "loss": 0.8286,
      "step": 2450
    },
    {
      "epoch": 0.869414014953921,
      "grad_norm": 5.685899257659912,
      "learning_rate": 0.00014216504404265183,
      "loss": 0.7777,
      "step": 2500
    },
    {
      "epoch": 0.8868022952529995,
      "grad_norm": 7.970619201660156,
      "learning_rate": 0.00014100602688919798,
      "loss": 0.7813,
      "step": 2550
    },
    {
      "epoch": 0.9041905755520779,
      "grad_norm": 5.464079856872559,
      "learning_rate": 0.0001398470097357441,
      "loss": 0.7783,
      "step": 2600
    },
    {
      "epoch": 0.9215788558511563,
      "grad_norm": 6.001948833465576,
      "learning_rate": 0.00013868799258229023,
      "loss": 0.7613,
      "step": 2650
    },
    {
      "epoch": 0.9389671361502347,
      "grad_norm": 5.329464435577393,
      "learning_rate": 0.00013752897542883635,
      "loss": 0.7418,
      "step": 2700
    },
    {
      "epoch": 0.9563554164493132,
      "grad_norm": 7.306352615356445,
      "learning_rate": 0.00013636995827538248,
      "loss": 0.7389,
      "step": 2750
    },
    {
      "epoch": 0.9737436967483916,
      "grad_norm": 5.178748607635498,
      "learning_rate": 0.0001352109411219286,
      "loss": 0.7439,
      "step": 2800
    },
    {
      "epoch": 0.99113197704747,
      "grad_norm": 4.998504638671875,
      "learning_rate": 0.00013405192396847475,
      "loss": 0.73,
      "step": 2850
    },
    {
      "epoch": 1.0083463745435577,
      "grad_norm": 7.0133795738220215,
      "learning_rate": 0.00013289290681502087,
      "loss": 0.7167,
      "step": 2900
    },
    {
      "epoch": 1.025734654842636,
      "grad_norm": 5.730682373046875,
      "learning_rate": 0.000131733889661567,
      "loss": 0.6772,
      "step": 2950
    },
    {
      "epoch": 1.0431229351417144,
      "grad_norm": 5.951767444610596,
      "learning_rate": 0.00013057487250811312,
      "loss": 0.6927,
      "step": 3000
    },
    {
      "epoch": 1.060511215440793,
      "grad_norm": 5.806336879730225,
      "learning_rate": 0.00012941585535465925,
      "loss": 0.6805,
      "step": 3050
    },
    {
      "epoch": 1.0778994957398713,
      "grad_norm": 5.129866600036621,
      "learning_rate": 0.00012825683820120537,
      "loss": 0.6751,
      "step": 3100
    },
    {
      "epoch": 1.0952877760389497,
      "grad_norm": 6.273079872131348,
      "learning_rate": 0.00012709782104775152,
      "loss": 0.687,
      "step": 3150
    },
    {
      "epoch": 1.1126760563380282,
      "grad_norm": 5.464356422424316,
      "learning_rate": 0.00012593880389429765,
      "loss": 0.6701,
      "step": 3200
    },
    {
      "epoch": 1.1300643366371066,
      "grad_norm": 5.800050735473633,
      "learning_rate": 0.00012477978674084377,
      "loss": 0.6623,
      "step": 3250
    },
    {
      "epoch": 1.147452616936185,
      "grad_norm": 6.0409111976623535,
      "learning_rate": 0.0001236207695873899,
      "loss": 0.6735,
      "step": 3300
    },
    {
      "epoch": 1.1648408972352635,
      "grad_norm": 5.641369342803955,
      "learning_rate": 0.00012246175243393602,
      "loss": 0.6508,
      "step": 3350
    },
    {
      "epoch": 1.1822291775343419,
      "grad_norm": 5.262079238891602,
      "learning_rate": 0.00012130273528048216,
      "loss": 0.6463,
      "step": 3400
    },
    {
      "epoch": 1.1996174578334202,
      "grad_norm": 5.612651824951172,
      "learning_rate": 0.0001201437181270283,
      "loss": 0.6307,
      "step": 3450
    },
    {
      "epoch": 1.2170057381324986,
      "grad_norm": 6.240873336791992,
      "learning_rate": 0.00011898470097357442,
      "loss": 0.6288,
      "step": 3500
    },
    {
      "epoch": 1.2343940184315771,
      "grad_norm": 6.215697288513184,
      "learning_rate": 0.00011782568382012054,
      "loss": 0.6445,
      "step": 3550
    },
    {
      "epoch": 1.2517822987306555,
      "grad_norm": 5.5405449867248535,
      "learning_rate": 0.00011666666666666668,
      "loss": 0.6216,
      "step": 3600
    },
    {
      "epoch": 1.2691705790297338,
      "grad_norm": 5.163707256317139,
      "learning_rate": 0.0001155076495132128,
      "loss": 0.6404,
      "step": 3650
    },
    {
      "epoch": 1.2865588593288124,
      "grad_norm": 5.688252925872803,
      "learning_rate": 0.00011434863235975893,
      "loss": 0.6146,
      "step": 3700
    },
    {
      "epoch": 1.3039471396278908,
      "grad_norm": 5.98386812210083,
      "learning_rate": 0.00011318961520630506,
      "loss": 0.629,
      "step": 3750
    },
    {
      "epoch": 1.3213354199269691,
      "grad_norm": 4.540277004241943,
      "learning_rate": 0.00011203059805285119,
      "loss": 0.5834,
      "step": 3800
    },
    {
      "epoch": 1.3387237002260477,
      "grad_norm": 5.665109157562256,
      "learning_rate": 0.00011087158089939731,
      "loss": 0.5831,
      "step": 3850
    },
    {
      "epoch": 1.356111980525126,
      "grad_norm": 5.6229705810546875,
      "learning_rate": 0.00010971256374594345,
      "loss": 0.5827,
      "step": 3900
    },
    {
      "epoch": 1.3735002608242044,
      "grad_norm": 4.98109245300293,
      "learning_rate": 0.00010855354659248957,
      "loss": 0.5847,
      "step": 3950
    },
    {
      "epoch": 1.390888541123283,
      "grad_norm": 6.927567005157471,
      "learning_rate": 0.0001073945294390357,
      "loss": 0.5791,
      "step": 4000
    }
  ],
  "logging_steps": 50,
  "max_steps": 8628,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.9914723218541773e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}

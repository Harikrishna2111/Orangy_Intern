{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7f7c48b3",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2mUsing Python 3.12.6 environment at: /usr/local\u001b[0m\r\n",
            "\u001b[2mAudited \u001b[1m6 packages\u001b[0m \u001b[2min 25ms\u001b[0m\u001b[0m\r\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%uv pip install transformers datasets peft accelerate bitsandbytes torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "997bb2f9",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# train.py\n",
        "# Fine-tune gemma-2-2b-chess using QLoRA\n",
        "# Task: Next-move prediction (Anand style)\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    BitsAndBytesConfig,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "19c72e8b",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Config\n",
        "# -------------------------\n",
        "MODEL_NAME = \"diabolic6045/gemma-2-2b-chess\"\n",
        "OUTPUT_DIR = \"anand_gemma_lora\"\n",
        "\n",
        "MAX_LENGTH = 256\n",
        "BATCH_SIZE = 8\n",
        "GRAD_ACCUM_STEPS = 2\n",
        "EPOCHS = 3\n",
        "LR = 2e-4\n",
        "\n",
        "SAVE_STEPS = 500        # üîê save every 500 steps\n",
        "SAVE_TOTAL_LIMIT = 3   # üîê keep only last 3 checkpoints\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f9659efd",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Load Dataset\n",
        "# -------------------------\n",
        "dataset = load_dataset(\n",
        "    \"text\",\n",
        "    data_files={\"train\": \"/root/anand_train.txt\"}\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "718ed4b3",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Tokenizer\n",
        "# -------------------------\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Gemma does not always define pad token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def tokenize_fn(example):\n",
        "    return tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "tokenized_ds = dataset.map(\n",
        "    tokenize_fn,\n",
        "    batched=True,\n",
        "    remove_columns=[\"text\"]\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "e6ba0f86",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
            "  warnings.warn(warning_msg)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# -------------------------\n",
        "# Quantization (QLoRA)\n",
        "# -------------------------\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "e1352873",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 1,597,440 || all params: 2,615,939,328 || trainable%: 0.0611\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# -------------------------\n",
        "# LoRA\n",
        "# -------------------------\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "5383c9ce",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Detected kernel version 4.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
          ]
        }
      ],
      "source": [
        "# -------------------------\n",
        "# Data Collator\n",
        "# -------------------------\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Training Arguments (CRASH-SAFE)\n",
        "# -------------------------\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    learning_rate=LR,\n",
        "\n",
        "    # üîê CHECKPOINTING\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=SAVE_STEPS,\n",
        "    save_total_limit=SAVE_TOTAL_LIMIT,\n",
        "\n",
        "    logging_steps=50,\n",
        "    fp16=True,\n",
        "    dataloader_num_workers=2,\n",
        "    report_to=\"none\",\n",
        "    optim=\"paged_adamw_8bit\"\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# Trainer\n",
        "# -------------------------\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_ds[\"train\"],\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# Resume logic (VERY IMPORTANT)\n",
        "# -------------------------\n",
        "checkpoint = None\n",
        "if os.path.isdir(OUTPUT_DIR):\n",
        "    checkpoints = [\n",
        "        os.path.join(OUTPUT_DIR, d)\n",
        "        for d in os.listdir(OUTPUT_DIR)\n",
        "        if d.startswith(\"checkpoint-\")\n",
        "    ]\n",
        "    if checkpoints:\n",
        "        checkpoint = sorted(checkpoints, key=os.path.getmtime)[-1]\n",
        "        print(f\"üîÅ Resuming training from checkpoint: {checkpoint}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "c995cc90",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4170' max='8628' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4170/8628 41:20 < 44:13, 1.68 it/s, Epoch 1.45/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.158800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.900500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.822900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.782800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.739400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.678900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.643900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.625900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>1.590200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.548100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>1.531300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.511200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>1.456400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.448200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>1.413300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.411400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>1.376600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.327400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>1.327200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.260000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>1.274100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>1.219000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>1.198000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>1.175200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>1.145400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>1.140700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>1.090700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>1.115100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>1.081600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>1.039400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1550</td>\n",
              "      <td>1.052600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>1.005300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1650</td>\n",
              "      <td>1.002700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.979400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1750</td>\n",
              "      <td>0.974800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.953000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1850</td>\n",
              "      <td>0.936300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.945800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1950</td>\n",
              "      <td>0.889200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.909300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2050</td>\n",
              "      <td>0.867400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.867200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2150</td>\n",
              "      <td>0.851400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.851400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>0.863700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.846500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2350</td>\n",
              "      <td>0.813500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.819300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2450</td>\n",
              "      <td>0.828600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.777700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2550</td>\n",
              "      <td>0.781300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.778300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2650</td>\n",
              "      <td>0.761300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.741800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2750</td>\n",
              "      <td>0.738900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>0.743900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2850</td>\n",
              "      <td>0.730000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>0.716700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2950</td>\n",
              "      <td>0.677200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.692700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3050</td>\n",
              "      <td>0.680500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3100</td>\n",
              "      <td>0.675100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3150</td>\n",
              "      <td>0.687000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>0.670100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3250</td>\n",
              "      <td>0.662300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3300</td>\n",
              "      <td>0.673500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3350</td>\n",
              "      <td>0.650800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3400</td>\n",
              "      <td>0.646300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3450</td>\n",
              "      <td>0.630700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.628800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3550</td>\n",
              "      <td>0.644500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>0.621600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3650</td>\n",
              "      <td>0.640400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3700</td>\n",
              "      <td>0.614600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3750</td>\n",
              "      <td>0.629000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3800</td>\n",
              "      <td>0.583400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3850</td>\n",
              "      <td>0.583100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3900</td>\n",
              "      <td>0.582700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3950</td>\n",
              "      <td>0.584700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.579100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4050</td>\n",
              "      <td>0.570700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4100</td>\n",
              "      <td>0.564600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4150</td>\n",
              "      <td>0.547600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/trainer.py:2328\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2326\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2327\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2328\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/trainer.py:2672\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2665\u001b[39m context = (\n\u001b[32m   2666\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2667\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2668\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2670\u001b[39m )\n\u001b[32m   2671\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2672\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2674\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2675\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2676\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2677\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2678\u001b[39m ):\n\u001b[32m   2679\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2680\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/trainer.py:4060\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   4057\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   4058\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4060\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4062\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/accelerate/accelerator.py:2730\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2728\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   2729\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.scaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2730\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2731\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.has_lomo_optimizer:\n\u001b[32m   2732\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# -------------------------\n",
        "# Train\n",
        "# -------------------------\n",
        "trainer.train(resume_from_checkpoint=checkpoint)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87cb5838",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Save final model\n",
        "# -------------------------\n",
        "model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "print(\"‚úÖ Training complete. Final model saved to:\", OUTPUT_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "55579380",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input moves : e4 e5\n",
            "Anand move  : c4\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# infer.py\n",
        "# Predict Anand's next move\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import PeftModel\n",
        "\n",
        "# -------------------------\n",
        "# Config\n",
        "# -------------------------\n",
        "BASE_MODEL = \"diabolic6045/gemma-2-2b-chess\"\n",
        "LORA_PATH = \"anand_gemma_lora/checkpoint-4000\"\n",
        "\n",
        "# -------------------------\n",
        "# Load Tokenizer\n",
        "# -------------------------\n",
        "tokenizer = AutoTokenizer.from_pretrained(LORA_PATH)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# -------------------------\n",
        "# Quantization config\n",
        "# -------------------------\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# Load Base Model\n",
        "# -------------------------\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# Attach LoRA Adapter\n",
        "# -------------------------\n",
        "model = PeftModel.from_pretrained(base_model, LORA_PATH)\n",
        "model.eval()\n",
        "\n",
        "# -------------------------\n",
        "# Prediction Function\n",
        "# -------------------------\n",
        "def predict_next_move(moves: str) -> str:\n",
        "    \"\"\"\n",
        "    Input:  'e4 e5 Nf3 Nc6'\n",
        "    Output: 'Bb5'\n",
        "    \"\"\"\n",
        "    prompt = f\"{moves} =>\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=6,\n",
        "            temperature=0.6,\n",
        "            top_k=10,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract only the predicted move\n",
        "    prediction = decoded.split(\"=>\")[-1].strip().split()[0]\n",
        "    return prediction\n",
        "\n",
        "# -------------------------\n",
        "# Test\n",
        "# -------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    test_input = \"e4 e5\"\n",
        "    print(\"Input moves :\", test_input)\n",
        "    print(\"Anand move  :\", predict_next_move(test_input))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
